[{"path":"https://hadley.github.io/elmer/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 elmer authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://hadley.github.io/elmer/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hadley Wickham. Author, maintainer. Posit Software, PBC. Copyright holder, funder.","code":""},{"path":"https://hadley.github.io/elmer/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wickham H (2024). elmer: Call LLM APIs R. R package version 0.0.0.9000, https://hadley.github.io/elmer/, https://github.com/hadley/elmer.","code":"@Manual{,   title = {elmer: Call LLM APIs from R},   author = {Hadley Wickham},   year = {2024},   note = {R package version 0.0.0.9000, https://hadley.github.io/elmer/},   url = {https://github.com/hadley/elmer}, }"},{"path":"https://hadley.github.io/elmer/index.html","id":"elmer-","dir":"","previous_headings":"","what":"Call LLM APIs from R","title":"Call LLM APIs from R","text":"goal elmer provide user friendly wrapper common APIs calling llm’s. Major design goals include support streaming making easy register call R functions.","code":""},{"path":"https://hadley.github.io/elmer/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Call LLM APIs from R","text":"can install development version elmer GitHub :","code":"# install.packages(\"pak\") pak::pak(\"hadley/elmer\")"},{"path":"https://hadley.github.io/elmer/index.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"Call LLM APIs from R","text":"use elmer, need OpenAI API key. can get one developer console. save value OPENAI_API_KEY environment variable ~/.Renviron (easy way open file call usethis::use_renviron()).","code":""},{"path":"https://hadley.github.io/elmer/index.html","id":"using-elmer","dir":"","previous_headings":"","what":"Using elmer","title":"Call LLM APIs from R","text":"chat elmer several different ways, depending whether working interactively programmatically. start creating new chat object: Chat objects stateful: retain context conversation, new query can build previous ones. true regardless various ways chatting use.","code":"library(elmer)  chat <- new_chat_openai(   model = \"gpt-4o-mini\",   system_prompt = \"You are a friendly but terse assistant.\",   echo = TRUE )"},{"path":"https://hadley.github.io/elmer/index.html","id":"interactive-chat-console","dir":"","previous_headings":"Using elmer","what":"Interactive chat console","title":"Call LLM APIs from R","text":"interactive, least programmatic way using elmer chat directly R console. chat console useful quickly exploring capabilities model, especially ’ve customized chat object tool integrations (see ). , keep mind chat object retains state, enter chat console, previous interactions chat object still part conversation, interactions chat console persist even exit back R prompt.","code":"chat$console() ╔════════════════════════════════════════════════════════╗ ║  Entering chat console. Use \"\"\" for multi-line input.  ║ ║  Press Ctrl+C to quit.                                 ║ ╚════════════════════════════════════════════════════════╝ >>> Who were the original creators of R? R was originally created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand.  >>> When was that? R was initially released in 1995. Development began a few years prior to that, in the early 1990s."},{"path":"https://hadley.github.io/elmer/index.html","id":"interactive-method-call","dir":"","previous_headings":"Using elmer","what":"Interactive method call","title":"Call LLM APIs from R","text":"second interactive way chat using elmer call chat() method. initialize chat object echo = TRUE, , chat method streams response console arrives. entire response received, returned character vector (invisibly, ’s printed twice). mode useful want see response arrives, don’t want enter chat console.","code":"chat$chat(\"What preceding languages most influenced R?\") R was primarily influenced by the S programming language, particularly S-PLUS. Other languages that had an impact include Scheme and various data analysis languages."},{"path":"https://hadley.github.io/elmer/index.html","id":"programmatic-chat","dir":"","previous_headings":"Using elmer","what":"Programmatic chat","title":"Call LLM APIs from R","text":"don’t want see response arrives, can turn echoing leaving echo = TRUE argument new_chat_openai(). mode useful programming using elmer, result either intended human consumption want process response displaying .","code":"chat <- new_chat_openai(   model = \"gpt-4o-mini\",   system_prompt = \"You are a friendly but terse assistant.\" ) chat$chat(\"Is R a functional programming language?\") [1] \"Yes, R supports functional programming concepts. It allows functions to be first-class objects, supports higher-order functions, and encourages the use of functions as core components of code. However, it also supports procedural and object-oriented programming styles.\""},{"path":"https://hadley.github.io/elmer/index.html","id":"streaming-results","dir":"","previous_headings":"Using elmer","what":"Streaming results","title":"Call LLM APIs from R","text":"chat() method return results entire response received. (can print streaming results console, returns result response complete.) want process response arrives, can use stream() method. may useful want display response realtime, somewhere R console (like writing file, HTTP response, Shiny chat window); want manipulate response displaying , without giving immediacy streaming. stream() method returns generator coro package, can loop process response arrives.","code":"stream <- chat$stream(\"What are some common uses of R?\") coro::loop(for (chunk in stream) {   cat(toupper(chunk)) }) R IS COMMONLY USED FOR:  1. **STATISTICAL ANALYSIS**: PERFORMING COMPLEX STATISTICAL TESTS AND ANALYSES. 2. **DATA VISUALIZATION**: CREATING GRAPHS, CHARTS, AND PLOTS USING PACKAGES LIKE GGPLOT2. 3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR AND TIDYR. 4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS WITH LIBRARIES LIKE CARET AND RANDOMFOREST. 5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA AND GENOMIC STUDIES. 6. **ECONOMETRICS**: PERFORMING ECONOMIC DATA ANALYSIS AND MODELING. 7. **REPORTING**: GENERATING DYNAMIC REPORTS AND DASHBOARDS WITH R MARKDOWN. 8. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA AND FORECASTING.  THESE USES MAKE R A POWERFUL TOOL FOR DATA SCIENTISTS, STATISTICIANS, AND RESEARCHERS."},{"path":"https://hadley.github.io/elmer/index.html","id":"async-usage-advanced","dir":"","previous_headings":"","what":"Async usage (Advanced)","title":"Call LLM APIs from R","text":"elmer also supports async usage, useful want run multiple chat sessions concurrently. primarily useful Shiny applications, using methods described block Shiny app users duration response. use async chat, instead chat()/stream(), call chat_async()/stream_async(). _async variants take arguments construction, return promises instead actual response. Remember chat objects stateful, maintaining conversation history interact . Note means doesn’t make sense issue multiple chat/stream operations chat object concurrently, conversation history become corrupted interleaved conversation fragments. need run multiple chat sessions concurrently, create multiple chat objects.","code":""},{"path":"https://hadley.github.io/elmer/index.html","id":"asynchronous-chat","dir":"","previous_headings":"Async usage (Advanced)","what":"Asynchronous chat","title":"Call LLM APIs from R","text":"asynchronous, non-streaming chat, use chat() method , handle result promise instead string. TODO: Shiny example","code":"library(promises)  chat$chat_async(\"How's your day going?\") %...>% print() I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions you have."},{"path":"https://hadley.github.io/elmer/index.html","id":"asynchronous-streaming","dir":"","previous_headings":"Async usage (Advanced)","what":"Asynchronous streaming","title":"Call LLM APIs from R","text":"asynchronous streaming, use stream() method , result async generator coro package. regular generator, except instead giving strings, gives promises resolve strings. Async generators advanced, require good understanding asynchronous programming R. also way present streaming results Shiny without blocking users. Fortunately, Shiny soon chat components make easier, can simply hand result stream_async() chat output.","code":"stream <- chat$stream_async(\"What are some common uses of R?\") coro::async(function() {   for (chunk in await_each(stream)) {     cat(toupper(chunk))   } })() <Promise [pending]> > R IS COMMONLY USED FOR:  1. **STATISTICAL ANALYSIS**: PERFORMING VARIOUS STATISTICAL TESTS AND MODELS. 2. **DATA VISUALIZATION**: CREATING PLOTS AND GRAPHS TO VISUALIZE DATA. 3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR. 4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS AND ALGORITHMS. 5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA, ESPECIALLY IN GENOMICS. 6. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA FOR TRENDS AND FORECASTS. 7. **REPORT GENERATION**: CREATING DYNAMIC REPORTS WITH R MARKDOWN. 8. **GEOSPATIAL ANALYSIS**: MAPPING AND ANALYZING GEOGRAPHIC DATA."},{"path":"https://hadley.github.io/elmer/index.html","id":"tool-integrations","dir":"","previous_headings":"","what":"Tool integrations","title":"Call LLM APIs from R","text":"TODO","code":""},{"path":"https://hadley.github.io/elmer/reference/elmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"elmer: Call LLM APIs from R — elmer-package","title":"elmer: Call LLM APIs from R — elmer-package","text":"consistent interface calling LLM APIs. Includes support streaming.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/elmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"elmer: Call LLM APIs from R — elmer-package","text":"Maintainer: Hadley Wickham hadley@posit.co contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"function returns R6 object takes care managing state associated chat; .e. records messages send server, messages receive back. register tool (aka R function), also takes care tool loop.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"new_chat_openai(   system_prompt = NULL,   messages = NULL,   base_url = \"https://api.openai.com/v1\",   api_key = openai_key(),   model = NULL,   echo = FALSE )"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"system_prompt system prompt set behavior assistant. messages list messages start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values messages system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. model model use chat; set NULL (default) use reasonable model, currently gpt-4o-mini. strongly recommend explicitly choosing model casual use. echo TRUE, chat() method streams response stdout default. (Note effect stream(), chat_async(), stream_async() methods.)","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"promise resolves string (probably Markdown).","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"ChatOpenAI$new() ChatOpenAI$messages() ChatOpenAI$chat() ChatOpenAI$chat_async() ChatOpenAI$stream() ChatOpenAI$stream_async() ChatOpenAI$console() ChatOpenAI$register_tool() ChatOpenAI$clone()","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"ChatOpenAI$new(base_url, model, messages, api_key, echo = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"base_url base URL endpoint; default uses ChatGPT. model model use chat; defaults GPT-4o mini. messages unnamed list messages start chat (.e., continuing previous conversation). NULL zero-length list, conversation begins scratch. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. echo TRUE, chat() method streams response stdout (also returning final response). Note effect stream(), chat_async(), stream_async() methods.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"method-messages-","dir":"Reference","previous_headings":"","what":"Method messages()","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"messages sent received far (starting system prompt, ).","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"ChatOpenAI$messages()"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"Submit text chatbot, return response simple string (probably Markdown).","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"ChatOpenAI$chat(text, echo = NULL)"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"text text send chatbot. echo Whether emit response stdout received. NULL, value echo set chat object created used.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"method-chat-async-","dir":"Reference","previous_headings":"","what":"Method chat_async()","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"Submit text chatbot, receive promise resolves response .","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"ChatOpenAI$chat_async(text)"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"text text send chatbot.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"method-stream-","dir":"Reference","previous_headings":"","what":"Method stream()","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"Submit text chatbot, returning streaming results. Returns coro generator yields strings. iterating, generator block waiting content chatbot.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"ChatOpenAI$stream(text)"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"text text send chatbot.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"method-stream-async-","dir":"Reference","previous_headings":"","what":"Method stream_async()","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"Submit text chatbot, returning asynchronously streaming results. Returns coro async generator yields string promises.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"ChatOpenAI$stream_async(text)"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"text text send chatbot.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"method-console-","dir":"Reference","previous_headings":"","what":"Method console()","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"Enter interactive chat console. REPL-like interface can chat assistant real-time. (available interactive() mode.)","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"ChatOpenAI$console()"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"method-register-tool-","dir":"Reference","previous_headings":"","what":"Method register_tool()","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"Register tool (R function) chatbot can use. chatbot decides use function, elmer automatically call submit results back.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"ChatOpenAI$register_tool(fun, name, description, arguments, strict = TRUE)"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"fun function invoked tool called. name name function. description detailed description function . Generally, information can provide , better. arguments named list arguments function accepts. named list objects created tool_arg(). strict argument definition strictly enforced?","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"objects class cloneable method.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"ChatOpenAI$clone(deep = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"deep Whether make deep clone.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"if (FALSE) { # elmer:::openai_key_exists() chat <- new_chat_openai() chat$chat(\"   What is the difference between a tibble and a data frame?   Answer with a bulleted list \")  chat <- new_chat_openai() chat$register_tool(   name = \"rnorm\",   description = \"Drawn numbers from a random normal distribution\",   arguments = list(     n = tool_arg(       type = \"integer\",       description = \"The number of observations. Must be a positive integer.\"     ),     mean = tool_arg(       type = \"number\",       description = \"The mean value of the distribution.\"     ),     sd = tool_arg(       type = \"number\",       description = \"The standard deviation of the distribution. Must be a non-negative number.\"     )   ) ) chat$chat(\"   Give me five numbers from a random normal distribution.   Briefly explain your work. \") }"},{"path":"https://hadley.github.io/elmer/reference/tool_arg.html","id":null,"dir":"Reference","previous_headings":"","what":"Define arguments for a tool — tool_arg","title":"Define arguments for a tool — tool_arg","text":"Define arguments tool","code":""},{"path":"https://hadley.github.io/elmer/reference/tool_arg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define arguments for a tool — tool_arg","text":"","code":"tool_arg(type, description, required = TRUE)"},{"path":"https://hadley.github.io/elmer/reference/tool_arg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define arguments for a tool — tool_arg","text":"type Argument type. description Description argument free text. required argument required?","code":""}]
